{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "context_length = 36\n",
    "embedding_dim = 72\n",
    "num_heads = 6\n",
    "head_dim = embedding_dim // num_heads\n",
    "num_layers = 6\n",
    "dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim, mask=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.mask = mask\n",
    "        if self.mask:\n",
    "            self.register('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "\n",
    "        B, T, C = embeddings.shape\n",
    "\n",
    "        key = self.key(embeddings) # (B, T, C)\n",
    "        query = self.query(embeddings) # (B, T, C)\n",
    "\n",
    "        # compute the weigts or scores\n",
    "        wei = query @ key.transpose(-2, -1) * C**-0.5 # (B, T, T)\n",
    "        if self.mask:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B, T, T)\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        value = self.value(embeddings) # (B, T, C)\n",
    "        output = wei @ value # (B, T, T) * (B, T, C) -> (B, T, C)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_dim, mask=False, cross_head = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(head_dim, mask) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "\n",
    "        output = torch.concat([head(embeddings) for head in self.heads], dim=-1) # concat along last dimension b/c the original embedding_dim is divided into n_heads times, each of size head_dim\n",
    "        output = self.dropout(self.proj(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        return self.ffn(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_mha = MultiHeadAttention(num_heads, head_dim, mask=False)\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        output = embeddings + self.ln1(self.self_mha(embeddings))\n",
    "        output = output + self.ln2(self.ffwd(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoders = nn.Sequential(*[EncoderLayer(embedding_dim, num_heads) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "\n",
    "        output = self.encoders(inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.8216e+00,  1.8666e+00,  6.8943e+00,  ..., -1.4715e+00,\n",
       "          -1.7789e+00, -4.2688e+00],\n",
       "         [ 3.8751e+00, -6.1958e+00,  4.7511e+00,  ...,  4.7486e+00,\n",
       "          -4.3046e-04, -1.5259e+00],\n",
       "         [ 1.9800e+00, -6.3879e-01,  6.6451e+00,  ...,  3.6647e+00,\n",
       "           3.9955e+00, -5.2388e+00],\n",
       "         ...,\n",
       "         [ 3.2471e+00, -5.4973e+00,  4.0067e+00,  ...,  2.5814e+00,\n",
       "          -1.1707e+00, -3.1103e+00],\n",
       "         [ 3.7453e+00, -7.0919e+00,  3.0443e+00,  ...,  8.3878e-01,\n",
       "           7.8253e-01, -3.3646e+00],\n",
       "         [ 1.2099e+00, -5.6506e+00,  5.8804e+00,  ...,  1.7355e+00,\n",
       "           1.6730e+00, -3.5347e+00]],\n",
       "\n",
       "        [[ 3.0355e+00, -5.7101e+00,  6.4742e+00,  ..., -3.0717e+00,\n",
       "           4.1893e+00, -7.8306e+00],\n",
       "         [ 1.1288e+00, -2.5011e-01,  4.1230e+00,  ..., -9.0750e-01,\n",
       "           2.8177e+00, -5.2242e+00],\n",
       "         [ 1.6335e+00, -4.4204e+00,  6.5933e+00,  ..., -1.0236e+00,\n",
       "           2.6325e+00, -7.5299e+00],\n",
       "         ...,\n",
       "         [ 6.0442e+00, -4.2704e+00,  2.8251e+00,  ...,  2.5959e+00,\n",
       "           4.6956e-01, -7.7793e+00],\n",
       "         [ 1.4420e+00, -3.5463e+00,  6.2625e+00,  ...,  4.2893e+00,\n",
       "           3.2520e+00, -7.1658e+00],\n",
       "         [ 1.1164e+00, -2.2383e-01,  2.1679e+00,  ...,  1.8549e+00,\n",
       "           1.8365e+00, -6.2003e+00]],\n",
       "\n",
       "        [[ 2.1984e+00, -3.4897e+00,  6.5315e+00,  ...,  8.4894e-01,\n",
       "          -4.8526e+00, -4.6199e+00],\n",
       "         [ 4.5181e+00, -9.6958e-01,  5.0619e+00,  ..., -5.4669e-01,\n",
       "          -3.8750e-01, -4.7886e+00],\n",
       "         [ 3.0956e+00, -4.0118e+00,  2.0919e+00,  ...,  2.9796e+00,\n",
       "          -2.2766e+00, -2.5911e+00],\n",
       "         ...,\n",
       "         [ 3.0494e+00, -1.8001e+00,  7.0712e+00,  ...,  1.9028e+00,\n",
       "          -2.2223e+00, -7.0520e+00],\n",
       "         [ 3.3173e+00, -7.6562e+00,  2.4136e+00,  ...,  2.6295e+00,\n",
       "          -2.9310e-02, -5.9180e+00],\n",
       "         [ 3.1492e+00, -2.5947e+00,  6.2410e+00,  ...,  2.6142e+00,\n",
       "          -2.1456e+00, -6.8279e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.8218e+00, -6.6610e-01,  6.7231e+00,  ...,  5.7701e+00,\n",
       "           1.4075e+00, -2.5522e+00],\n",
       "         [ 5.0435e+00,  8.8085e-01,  8.8033e+00,  ...,  2.2553e+00,\n",
       "           8.3798e-01, -1.9211e-02],\n",
       "         [ 3.5560e+00, -4.0617e+00,  6.5336e+00,  ...,  1.1274e+00,\n",
       "          -4.5573e-01, -4.9764e+00],\n",
       "         ...,\n",
       "         [ 2.4548e+00, -7.2055e-02,  6.2229e+00,  ...,  6.0171e-01,\n",
       "           1.9960e+00, -5.6456e+00],\n",
       "         [ 5.5754e+00, -1.6670e+00,  4.5909e+00,  ...,  3.7439e+00,\n",
       "           6.7013e+00, -2.2225e+00],\n",
       "         [ 5.0535e+00, -2.5269e+00,  5.3171e+00,  ...,  1.4177e+00,\n",
       "          -1.2309e+00, -4.3733e+00]],\n",
       "\n",
       "        [[ 1.6063e+00, -3.1983e+00,  4.9483e+00,  ...,  6.6021e+00,\n",
       "           5.0884e-01, -3.5685e+00],\n",
       "         [ 2.0656e+00, -2.6911e+00, -1.2914e+00,  ...,  2.9284e+00,\n",
       "          -1.1549e+00,  1.0214e+00],\n",
       "         [ 5.5794e+00, -1.3556e+00,  5.5074e+00,  ...,  3.7812e+00,\n",
       "           4.3002e+00, -3.0230e+00],\n",
       "         ...,\n",
       "         [ 1.4079e+00, -6.4138e+00,  8.6892e+00,  ...,  4.9593e+00,\n",
       "           3.5510e+00, -1.7743e-02],\n",
       "         [ 3.2572e+00,  3.6621e-01,  1.3648e+00,  ...,  3.8592e+00,\n",
       "          -6.3768e-02,  2.0774e-01],\n",
       "         [ 2.5914e+00, -3.4652e+00,  4.4010e+00,  ..., -8.2305e-01,\n",
       "           2.7674e+00, -5.5769e+00]],\n",
       "\n",
       "        [[ 5.3093e+00, -2.1227e+00, -1.5778e+00,  ...,  1.4420e+00,\n",
       "           1.7251e+00, -7.7676e-01],\n",
       "         [ 4.5115e+00, -2.3587e+00,  6.5748e+00,  ...,  4.3015e+00,\n",
       "          -2.5788e-01, -6.5617e-01],\n",
       "         [ 3.1080e+00, -2.9869e+00,  3.2154e+00,  ...,  4.5690e+00,\n",
       "           7.4911e-01,  2.5947e-01],\n",
       "         ...,\n",
       "         [ 4.1802e+00,  2.5781e+00,  1.8436e+00,  ...,  1.3850e+00,\n",
       "           3.3296e+00, -5.2744e+00],\n",
       "         [ 5.8673e+00,  2.4569e+00,  2.9622e+00,  ..., -7.5204e-03,\n",
       "           1.0245e+00, -4.1150e+00],\n",
       "         [ 5.7949e+00,  3.4572e+00,  2.9827e+00,  ...,  4.9274e+00,\n",
       "          -1.9450e-01, -2.5508e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.normal(mean=0.0, std=1.0, size=(batch_size, context_length, embedding_dim))\n",
    "encoder = Encoder(num_layers)\n",
    "encoder(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-fall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
