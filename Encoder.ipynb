{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "context_length = 36\n",
    "embedding_dim = 72\n",
    "num_heads = 6\n",
    "head_dim = embedding_dim // num_heads\n",
    "num_layers = 6\n",
    "dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim, mask=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.mask = mask\n",
    "        if self.mask:\n",
    "            self.register('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "\n",
    "        B, T, C = embeddings.shape\n",
    "\n",
    "        key = self.key(embeddings) # (B, T, C)\n",
    "        query = self.query(embeddings) # (B, T, C)\n",
    "\n",
    "        # compute the weigts or scores\n",
    "        wei = query @ key.transpose(-2, -1) * C**-0.5 # (B, T, T)\n",
    "        if self.mask:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B, T, T)\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        value = self.value(embeddings) # (B, T, C)\n",
    "        output = wei @ value # (B, T, T) * (B, T, C) -> (B, T, C)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_dim, mask=False, cross_head = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(head_dim, mask) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "\n",
    "        output = torch.concat([head(embeddings) for head in self.heads], dim=-1) # concat along last dimension b/c the original embedding_dim is divided into n_heads times, each of size head_dim\n",
    "        output = self.dropout(self.proj(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        return self.ffn(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_mha = MultiHeadAttention(num_heads, head_dim, mask=False)\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        output = embeddings + self.ln1(self.self_mha(embeddings))\n",
    "        output = output + self.ln2(self.ffwd(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6426,  0.7637,  1.4020,  ..., -1.4449, -0.8595,  1.2576],\n",
       "         [-0.5024, -0.9455, -0.0903,  ..., -2.3918, -2.6005, -2.4849],\n",
       "         [-0.9345,  2.1450, -0.6613,  ...,  2.1165, -1.9769, -3.4450],\n",
       "         ...,\n",
       "         [-0.9818, -0.7390, -0.7188,  ..., -0.6580,  0.1123, -1.3307],\n",
       "         [ 0.1014, -0.3141,  1.5314,  ..., -2.5026,  0.3968,  2.0615],\n",
       "         [-1.4914, -0.8695,  0.5274,  ..., -1.8936, -0.6168, -4.8527]],\n",
       "\n",
       "        [[ 1.8072,  0.7790, -0.8845,  ...,  0.2791, -2.7844,  2.8419],\n",
       "         [ 0.0472, -2.3096,  0.1101,  ..., -3.2924, -1.3823,  1.8936],\n",
       "         [-0.6778, -2.4964,  0.6566,  ..., -0.7618, -3.4384, -0.2994],\n",
       "         ...,\n",
       "         [ 1.6338, -1.4597,  1.5779,  ..., -1.1601, -0.7695, -1.7222],\n",
       "         [ 0.9130, -0.3275,  3.5388,  ..., -1.5530, -3.3836,  0.9193],\n",
       "         [ 1.5087, -0.8400,  1.3757,  ...,  2.0936, -4.1108, -1.4147]],\n",
       "\n",
       "        [[-0.0892, -0.5195,  1.3635,  ...,  0.4821, -0.9348,  2.0546],\n",
       "         [-0.0410,  1.4158,  3.3364,  ..., -0.7398, -3.1592, -1.1547],\n",
       "         [-0.4026,  1.5789,  0.9369,  ..., -0.4779, -3.9274, -1.7118],\n",
       "         ...,\n",
       "         [ 0.2145, -1.1288,  2.3377,  ...,  0.7654, -2.7305,  0.2710],\n",
       "         [-2.4092, -0.9672,  0.8188,  ..., -0.1393, -2.9801,  2.3483],\n",
       "         [-0.1398,  0.7076,  2.3728,  ..., -1.3873, -1.1725,  1.7154]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6414, -3.0514,  2.0995,  ..., -2.5140, -2.7346, -1.2889],\n",
       "         [-2.7662,  1.9181,  0.7601,  ..., -3.4969, -2.4921,  1.5292],\n",
       "         [ 1.6810, -0.1813,  2.1343,  ..., -0.2302, -4.3274,  0.7393],\n",
       "         ...,\n",
       "         [-0.0641, -2.0151,  0.3247,  ..., -1.8761, -4.4339, -2.0421],\n",
       "         [-0.5934,  0.7650, -1.1312,  ..., -1.7742, -3.8653, -0.9677],\n",
       "         [ 1.0582, -1.9757,  1.5188,  ..., -2.2764, -5.3726, -0.1025]],\n",
       "\n",
       "        [[ 2.0195, -1.7694, -0.4470,  ..., -3.3627, -3.8660, -0.0464],\n",
       "         [ 0.1025, -2.1859,  2.3706,  ..., -1.2656, -4.4048, -0.7988],\n",
       "         [ 2.5424, -1.1047,  2.1613,  ...,  0.7544, -3.7490, -0.5077],\n",
       "         ...,\n",
       "         [ 1.3553, -2.3706,  1.1860,  ..., -0.4691, -3.7044, -0.3716],\n",
       "         [ 3.4402, -2.1682,  0.6812,  ...,  1.4305, -0.5645,  1.2856],\n",
       "         [ 0.1918, -1.9451,  2.0981,  ...,  0.5354, -5.0056, -1.6246]],\n",
       "\n",
       "        [[ 0.0922,  1.5091,  2.7379,  ..., -1.2848, -2.6188, -1.0366],\n",
       "         [ 1.1158, -3.5875,  0.5189,  ..., -0.7268, -1.9531,  0.4299],\n",
       "         [-0.1396,  1.5983, -0.8989,  ...,  0.4373, -2.3590, -0.2050],\n",
       "         ...,\n",
       "         [-0.7900,  0.4948,  1.8006,  ..., -2.0764, -2.2430, -1.9510],\n",
       "         [-0.5300, -1.1070,  1.0972,  ..., -0.8207, -3.7589, -3.3768],\n",
       "         [ 0.1616, -3.2752,  1.2916,  ...,  1.3432, -1.7402, -1.5733]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.normal(mean=0.0, std=1.0, size=(batch_size, context_length, embedding_dim))\n",
    "encoder = Encoder(embedding_dim, num_heads)\n",
    "encoder(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-fall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
