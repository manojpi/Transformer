{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "context_length = 36\n",
    "embedding_dim = 72\n",
    "num_heads = 6\n",
    "head_dim = embedding_dim // num_heads\n",
    "num_layers = 6\n",
    "dropout=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_dim, mask=False, cross_head = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.query = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, head_dim, bias=False)\n",
    "        self.mask = mask\n",
    "\n",
    "        if self.mask:\n",
    "            self.register_buffer(\"tril\", torch.tril(torch.ones(context_length, context_length)))\n",
    "    \n",
    "    def forward(self, embeddings, encoder_embeddings = None):\n",
    "\n",
    "        B, T, C = embeddings.shape\n",
    "\n",
    "        key = self.key(encoder_embeddings) if encoder_embeddings is not None else self.key(embeddings)\n",
    "        value = self.value(encoder_embeddings)  if encoder_embeddings is not None else self.value(embeddings)\n",
    "        query = self.query(embeddings)\n",
    "\n",
    "        wei = query @ key.transpose(-2, -1) * C ** -0.5\n",
    "        if self.mask:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        value = self.value(embeddings)\n",
    "        output = wei @ value\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(head_dim, mask=True) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "\n",
    "        output = torch.concat([head(embeddings) for head in self.heads], dim=-1)\n",
    "        output = self.dropout(self.proj(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCrossHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(head_dim) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, embeddings, encoder_embeddings):\n",
    "\n",
    "        output = torch.concat([head(embeddings, encoder_embeddings) for head in self.heads], dim=-1)\n",
    "        output = self.dropout(self.proj(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        return self.ffn(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_mha = MultiHeadAttention(num_heads, head_dim)\n",
    "        self.cross_mha = MultiCrossHeadAttention(num_heads, head_dim)\n",
    "        self.ffwd = FeedForward(embedding_dim)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "        self.ln3 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, embeddings, encoder_embeddings):\n",
    "\n",
    "        output = embeddings + self.ln1(self.self_mha(embeddings))\n",
    "        output = output + self.ln2(self.cross_mha(output, encoder_embeddings))\n",
    "        output = output + self.ln3(self.ffwd(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers):\n",
    "        super().__init__()\n",
    "        self.decoders = [DecoderLayer(head_dim, embedding_dim) for _ in range(num_layers)]\n",
    "        self.final_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, embeddings, encoder_embeddings):\n",
    "        output = embeddings\n",
    "        for decoder in self.decoders:\n",
    "            output = decoder(output, encoder_embeddings)\n",
    "        output = self.final_linear(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3666e+00,  1.4253e+00,  2.7469e+00,  ..., -6.0862e-01,\n",
       "           3.5372e-01,  2.6608e+00],\n",
       "         [ 1.4086e+00,  3.7723e+00,  6.8644e-01,  ..., -2.8146e+00,\n",
       "          -1.7313e+00,  1.5454e+00],\n",
       "         [ 1.1342e+00,  4.5126e+00,  2.7331e+00,  ..., -3.9873e+00,\n",
       "          -4.7525e-01,  2.9878e+00],\n",
       "         ...,\n",
       "         [-3.6420e-01,  1.9867e+00,  3.5650e+00,  ..., -2.8463e+00,\n",
       "           3.5189e-01, -1.6098e-01],\n",
       "         [-2.6925e+00,  4.9364e-01,  3.6550e+00,  ..., -1.6877e+00,\n",
       "          -2.0918e+00,  1.0198e+00],\n",
       "         [-1.7929e+00,  3.0967e+00,  3.2468e+00,  ..., -1.3510e+00,\n",
       "           2.7414e-01,  2.5220e+00]],\n",
       "\n",
       "        [[ 4.5378e+00,  2.5681e+00, -1.7457e+00,  ...,  2.6736e+00,\n",
       "          -1.9984e+00,  1.3154e+00],\n",
       "         [ 1.7903e+00,  2.3349e+00, -6.8890e-01,  ...,  2.0767e+00,\n",
       "          -1.4876e+00, -4.5297e-01],\n",
       "         [ 6.3503e-01,  2.8398e+00,  1.4644e-01,  ...,  1.3603e+00,\n",
       "          -2.4791e+00,  1.0748e+00],\n",
       "         ...,\n",
       "         [ 2.8744e+00,  6.8852e-01,  1.0200e+00,  ...,  3.4700e-01,\n",
       "          -1.3508e+00,  1.5520e+00],\n",
       "         [ 1.3550e+00,  1.2680e+00, -2.4431e-01,  ..., -2.1733e-01,\n",
       "          -3.2483e+00, -2.9731e-01],\n",
       "         [ 2.2219e+00,  3.7614e-01,  5.3649e-03,  ...,  3.1084e+00,\n",
       "          -4.2018e-01,  1.1308e+00]],\n",
       "\n",
       "        [[-7.3530e-01,  3.9040e+00, -3.7870e+00,  ..., -1.4388e+00,\n",
       "          -4.0268e+00,  2.9328e+00],\n",
       "         [-4.1455e-01,  1.1918e+00, -1.3087e+00,  ..., -9.3945e-01,\n",
       "          -1.5566e+00,  7.0409e-02],\n",
       "         [-2.0688e-01,  3.5168e+00, -9.5880e-01,  ..., -7.0116e-01,\n",
       "          -1.8244e+00, -9.6243e-02],\n",
       "         ...,\n",
       "         [-1.3047e+00,  1.4225e+00, -3.2359e+00,  ..., -1.0016e+00,\n",
       "          -2.2494e+00, -2.7610e+00],\n",
       "         [-5.2629e+00,  5.2463e+00,  1.0753e+00,  ..., -1.5420e+00,\n",
       "          -2.2677e+00,  2.2903e+00],\n",
       "         [-4.4172e-01,  1.3772e+00, -2.9023e+00,  ...,  1.2448e+00,\n",
       "          -7.6267e-01, -1.0208e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.6961e+00,  1.3923e+00, -3.2437e+00,  ..., -1.7416e+00,\n",
       "          -6.6323e+00,  9.3104e-01],\n",
       "         [ 3.2537e+00,  9.7834e-01, -3.5268e+00,  ...,  2.9284e-01,\n",
       "          -5.0278e+00,  1.4255e+00],\n",
       "         [ 2.7338e+00,  1.1831e+00, -2.8083e+00,  ...,  9.3428e-01,\n",
       "          -5.6496e+00,  1.7247e+00],\n",
       "         ...,\n",
       "         [-2.9163e+00,  4.4858e+00, -4.5411e-01,  ..., -2.2232e+00,\n",
       "          -6.8772e+00, -1.1153e+00],\n",
       "         [-2.8632e-02,  2.1479e+00, -4.9121e+00,  ..., -2.2209e+00,\n",
       "          -6.9524e+00,  1.8958e+00],\n",
       "         [-1.7164e+00,  1.3246e+00, -3.4429e+00,  ..., -3.5493e+00,\n",
       "          -5.1755e+00, -9.6344e-03]],\n",
       "\n",
       "        [[ 2.3599e+00, -6.4443e-01, -1.5808e+00,  ...,  1.3841e+00,\n",
       "           2.2915e-01, -2.9725e+00],\n",
       "         [-7.7549e-01,  9.1143e-01, -2.0911e+00,  ..., -3.8071e+00,\n",
       "          -5.0318e+00, -1.9389e+00],\n",
       "         [-2.9025e-01,  3.8242e+00,  7.1437e-02,  ..., -1.3094e+00,\n",
       "          -3.2306e+00,  2.1208e-02],\n",
       "         ...,\n",
       "         [-6.3939e-01,  7.0730e+00, -1.3241e-01,  ..., -1.3719e+00,\n",
       "          -2.6112e+00,  2.3197e+00],\n",
       "         [-5.7928e+00,  6.7902e+00,  1.4183e+00,  ..., -8.5305e-01,\n",
       "          -4.4075e+00, -1.7367e-01],\n",
       "         [-2.6843e+00,  5.8008e+00, -3.4558e+00,  ...,  8.8717e-01,\n",
       "          -2.2720e+00,  1.0375e+00]],\n",
       "\n",
       "        [[ 1.8580e+00,  4.1884e+00, -2.4091e+00,  ...,  1.2661e+00,\n",
       "          -3.0417e+00, -1.7773e+00],\n",
       "         [ 2.6500e+00,  4.3438e+00, -1.9513e+00,  ...,  1.2336e+00,\n",
       "          -4.4395e+00, -3.1450e-01],\n",
       "         [ 2.8474e+00,  4.2408e+00,  1.4039e+00,  ...,  6.6308e-01,\n",
       "          -2.1878e+00,  4.2914e+00],\n",
       "         ...,\n",
       "         [-1.9603e+00,  2.1022e+00,  8.6553e-01,  ..., -2.5683e+00,\n",
       "          -3.0902e+00,  1.8744e+00],\n",
       "         [-5.4137e-01,  3.2544e+00, -1.7765e+00,  ..., -5.1204e-01,\n",
       "          -2.6830e+00,  1.5696e+00],\n",
       "         [-2.1193e+00,  2.8266e+00, -1.2820e+00,  ..., -1.8252e+00,\n",
       "          -3.7369e+00, -1.1013e+00]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.normal(mean=0.0, std=1.0, size=(batch_size, context_length, embedding_dim))\n",
    "y = torch.normal(mean=0.0, std=5.0, size=(batch_size, context_length, embedding_dim))\n",
    "decoder = Decoder(num_layers)\n",
    "output = decoder(x, y)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-fall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
